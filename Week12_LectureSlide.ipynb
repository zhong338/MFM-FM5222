{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhong338/MFM-FM5222/blob/main/Week12_LectureSlide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a789620",
      "metadata": {
        "id": "9a789620"
      },
      "source": [
        "# FM 5222 \n",
        "# Week 12\n",
        "\n",
        "\n",
        "Agenda\n",
        "\n",
        "1) Bayesian Posteriors\n",
        "\n",
        "2) Simulating from the posterior distribution\n",
        "\n",
        "3) Using pymc3 and examples\n",
        "\n",
        "4) Some observations about priors\n",
        "\n",
        "\n",
        "5) Fitting some actual data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbdca8f",
      "metadata": {
        "id": "0fbdca8f"
      },
      "source": [
        "## Bayesian Posteriors\n",
        "\n",
        "We usse Baysian posteriors is generally in the following context\n",
        "\n",
        "* We have data that we believe are generated from a known distribution but with $p$ unkown parameters.  \n",
        "\n",
        "\n",
        "\n",
        "* Though the parameters are unkown, we have some prior information on what they might be. We express this information as a prior distribution.\n",
        "\n",
        "\n",
        "\n",
        "Suppose our data is give by $y = (y_1,y_2,...,y_N)$ and their distribution is given by $f(x;\\theta_1,\\theta_2,...,\\theta_p)$\n",
        "\n",
        "\n",
        "The prior distribution (expressing what we know about the parameters) is given by\n",
        "\n",
        "$g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta_1, \\eta_2,...,\\eta_m) = g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta)$\n",
        "\n",
        "\n",
        "Here the parameters of prior distribution are denoted by $\\eta = (\\eta_1, \\eta_2,...,\\eta_m)$.  These are sometimes called *hyper-parameters* to avoid confusing them with the parameters of $f$.\n",
        "\n",
        "\n",
        "Given our observations, we can state the posterior distribution for the $\\theta$ parameters:\n",
        "\n",
        "\n",
        "$$g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p| y) = \\frac{\\left(\\Pi_{k=1}^n f(y_k;\\theta_1, \\theta_2,...,\\theta_p)\\right)g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta)}{\\int \\cdots \\int \\left(\\Pi_{k=1}^n f(y_k;\\theta_1, \\theta_2,...,\\theta_p)\\right)g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta) d\\theta_1 \\cdots d\\theta_p}$$\n",
        "\n",
        "\n",
        "We note that the messy integral in the denominator is just a number, and really only serves the purpose of ensuring that that the posterior actually is a pdf (it will integrate to 1).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2aafbd",
      "metadata": {
        "id": "ce2aafbd"
      },
      "source": [
        "#### Historically, working with Baysian posteriors was hard\n",
        "\n",
        "The calculations necessary to capture things like the mode (requires optimization) or the mean (requires integration) of the posterior and numerically tricky. It takes effort to not suffer from signifigant round-off error.\n",
        "\n",
        "\n",
        "On the other hand, if we restrict ourselves to conjugate priors, things are nice since we don't need to perform difficult numerical calculations.  But restricting to conjugate priors limits us to situations where they apply - and are understood.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e65c90d",
      "metadata": {
        "id": "9e65c90d"
      },
      "source": [
        "## Simulating the Posterior\n",
        "\n",
        "Instead of directly calculating items of interest (like mode and mean), what if we can generate large samples of data that follow the posterior distribution (note: this data will be  samples of the *parameters*).   Then, we can easily calculate the mean (sample mean) and typically can infer a mode using binning or a kde). \n",
        "\n",
        "\n",
        "The tool we have to do this is called Markov Chain Monte Carlo (MCMC).  \n",
        "\n",
        "We consider our expressino above:\n",
        "\n",
        "\n",
        "$$g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p| y) = \\frac{\\left(\\Pi_{k=1}^n f(y_k;\\theta_1, \\theta_2,...,\\theta_p)\\right)g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta)}{\\int \\cdots \\int \\left(\\Pi_{k=1}^n f(y_k;\\theta_1, \\theta_2,...,\\theta_p)\\right)g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta) d\\theta_1 \\cdots d\\theta_p}$$\n",
        "\n",
        "\n",
        "and note that we can write\n",
        "\n",
        "\n",
        "$$g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p| y) \\propto \\left(\\Pi_{k=1}^n f(y_k;\\theta_1, \\theta_2,...,\\theta_p)\\right)g_{\\Theta}(\\theta_1, \\theta_2,...,\\theta_p; \\eta)$$\n",
        "\n",
        "\n",
        "It turns out that we only need to define distributions up to a multiple in order to generate sample from it.  That is one of the key advantages of MCMC.\n",
        "\n",
        "\n",
        "\n",
        "How can this be done?  It turns out that there are several such algorithms. The most famous of which is the Metropolis-Hastings algorithm.\n",
        "\n",
        "There are other that can be faster provided the pdf's involved can be differentiated symbolically by a computer.  One we will soon see is called the \"No U Turn Sampler\" (NUTS).\n",
        "\n",
        "For now, I am going to treat them as black boxes and show you how to use a python package called pymc3 to generate such samples.\n",
        "\n",
        "\n",
        "Next week, I will give you a hint on how they work, most likely in a video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04ac980",
      "metadata": {
        "id": "d04ac980"
      },
      "source": [
        "## Using pymc3\n",
        "\n",
        "We start by loading packages and constructing an example to see the package can be used.\n",
        "\n",
        "\n",
        "\n",
        "FYI:   Documentation can be found [here](https://docs.pymc.io/en/v3/api.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install yfinance"
      ],
      "metadata": {
        "id": "yz7RRa0uo9OL"
      },
      "id": "yz7RRa0uo9OL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa69918",
      "metadata": {
        "id": "efa69918"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as ss\n",
        "\n",
        "import pymc3 as pm\n",
        "\n",
        "import arviz as az"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac640b01",
      "metadata": {
        "id": "ac640b01"
      },
      "source": [
        "### Example - coin flipping again\n",
        "\n",
        "\n",
        "We will generate 100 flips with $\\theta_{true} = .7$ and use pymc3 to understand the posterior.  We will set the prior to the uniform distribution on $[0,1]$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ef522c",
      "metadata": {
        "id": "c7ef522c"
      },
      "outputs": [],
      "source": [
        "# generating data\n",
        "N = 100\n",
        "theta_true = .7\n",
        "\n",
        "data = ss.binom.rvs(n = 1, p = theta_true, size = N, random_state = 1981)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2673ae",
      "metadata": {
        "id": "da2673ae"
      },
      "source": [
        "We next need to define a model in pymc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bbb02e",
      "metadata": {
        "id": "e5bbb02e"
      },
      "outputs": [],
      "source": [
        "model_1 = pm.Model()\n",
        "\n",
        "with model_1:\n",
        "    \n",
        "    #define the prior\n",
        "    \n",
        "    theta = pm.Uniform('theta', lower= 0, upper = 1)\n",
        "    \n",
        "    \n",
        "    #define the distribution of the observed variables\n",
        "    \n",
        "    Y_obs = pm.Bernoulli('Y_obs', p=theta ,  observed=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f2d86c",
      "metadata": {
        "id": "81f2d86c"
      },
      "source": [
        "Now we are ready to sample from the posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464d053a",
      "metadata": {
        "id": "464d053a"
      },
      "outputs": [],
      "source": [
        "with model_1:\n",
        "    \n",
        "    trace = pm.sample(1000)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e30d4f",
      "metadata": {
        "id": "50e30d4f"
      },
      "source": [
        "The number of chains is determined by how many processsors are available.  You can change it on your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beef5a64",
      "metadata": {
        "id": "beef5a64"
      },
      "outputs": [],
      "source": [
        "help(pm.sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4873c01",
      "metadata": {
        "id": "b4873c01"
      },
      "source": [
        "Let's take a look at the data that is stored in our trace object.  \n",
        "\n",
        "We can use the package arviz for this.  (note, pymc3 is now maintained by the arviz people)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905ddc80",
      "metadata": {
        "id": "905ddc80"
      },
      "outputs": [],
      "source": [
        "with model_1: \n",
        "    \n",
        "    az.plot_trace(trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861a6fe7",
      "metadata": {
        "id": "861a6fe7"
      },
      "source": [
        "What we want to see is that the kde's (one for each chain) look similar.  We also want the second plot to show stationarity.   In not, we may need to increese the tune parameter.\n",
        "\n",
        "\n",
        "Let's look at the PDF (note, this is actually a KDE) when we combine all the chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc288680",
      "metadata": {
        "id": "fc288680"
      },
      "outputs": [],
      "source": [
        "with model_1:\n",
        "\n",
        "  az.plot_posterior(trace, figsize = (15,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a831a1",
      "metadata": {
        "id": "87a831a1"
      },
      "source": [
        "We can also get a summary of the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4aa329",
      "metadata": {
        "id": "5d4aa329"
      },
      "outputs": [],
      "source": [
        "with model_1: \n",
        "    \n",
        "    sum1 = az.summary(trace)\n",
        "    \n",
        "sum1    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6386cebc",
      "metadata": {
        "id": "6386cebc"
      },
      "source": [
        "We can also directly access the sampled data if we don't want to use arviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67bf5e5d",
      "metadata": {
        "id": "67bf5e5d"
      },
      "outputs": [],
      "source": [
        "samples = trace['theta']\n",
        "\n",
        "type(samples), samples.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa703b5",
      "metadata": {
        "id": "6fa703b5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize= (15,10))\n",
        "\n",
        "plt.hist(samples, bins=50, edgecolor = \"black\", alpha = .6)\n",
        "\n",
        "plt.title(\"Histogram of sampled thetas\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e567427",
      "metadata": {
        "id": "4e567427"
      },
      "source": [
        "### Example 2\n",
        "\n",
        "In this example, we will generate data from the normal distribution with $\\mu = 2$ and $\\sigma = 2$.\n",
        "\n",
        "We will have two priors:\n",
        "\n",
        "For $\\mu$, we will take a uniform distribution on $[-5,5]$\n",
        "\n",
        "For $\\sigma$, we will take lognormal distribution with shape parameter $s= 0.5$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d61252",
      "metadata": {
        "id": "39d61252"
      },
      "source": [
        "Before we even generate data, let's plot the prior for $\\sigma$ (for $\\mu$, it's boring)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74e8636",
      "metadata": {
        "id": "d74e8636"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  5, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.lognorm.pdf(x, s= 0.5), label=\"prior for $\\sigma$\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for $\\sigma$\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b530305d",
      "metadata": {
        "id": "b530305d"
      },
      "source": [
        "Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ca1cec",
      "metadata": {
        "id": "08ca1cec"
      },
      "outputs": [],
      "source": [
        "N = 100\n",
        "data2 = ss.norm.rvs(loc = 2, scale = 2, size = N)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9f367e",
      "metadata": {
        "id": "1c9f367e"
      },
      "source": [
        "Set up our pymc3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16df6d02",
      "metadata": {
        "id": "16df6d02"
      },
      "outputs": [],
      "source": [
        "model_2 = pm.Model()\n",
        "\n",
        "with model_2:\n",
        "    \n",
        "    #define the priors\n",
        "    \n",
        "    mu = pm.Uniform('mu', lower= -5, upper = 5)\n",
        "    \n",
        "    sigma = pm.Lognormal('sigma', sigma = .5)\n",
        "    \n",
        "    \n",
        "    #define the distribution of the observed variables\n",
        "    \n",
        "    Y_obs = pm.Normal('Y_obs', mu = mu , sigma = sigma,  observed=data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab343e5",
      "metadata": {
        "id": "4ab343e5"
      },
      "outputs": [],
      "source": [
        "# Run the sampler\n",
        "\n",
        "\n",
        "with model_2:\n",
        "    \n",
        "    trace2 = pm.sample(1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a482e37",
      "metadata": {
        "id": "6a482e37"
      },
      "source": [
        "Now we look at what we see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7b2e25",
      "metadata": {
        "id": "6f7b2e25"
      },
      "outputs": [],
      "source": [
        "with model_2: \n",
        "    \n",
        "    az.plot_trace(trace2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a21218",
      "metadata": {
        "id": "46a21218"
      },
      "source": [
        "Let us compare our posterior for $sigma$ with the prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7592b84f",
      "metadata": {
        "id": "7592b84f"
      },
      "outputs": [],
      "source": [
        "sigma_kde =  ss.gaussian_kde(trace2['sigma'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201016b1",
      "metadata": {
        "id": "201016b1"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  5, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.lognorm.pdf(x, s= 0.5), label=\"prior for $\\sigma$\")\n",
        "\n",
        "plt.plot(x, sigma_kde(x), label=\"Posterior KDE from MCMC sample\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for $\\sigma$\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e18c2bb",
      "metadata": {
        "id": "0e18c2bb"
      },
      "source": [
        "A comment is in order here.\n",
        "\n",
        "As priors, we treated $\\mu$ and $\\sigma$ and indepedendent random variables.  The information above looks at the posteriors from a marginal perspective.  However, the data series actually represents the *joint* distribution of $\\mu$ and $\\sigma$.  Even if they are originally indepdendent, the posterior distribtion will not be (necessarily).\n",
        "\n",
        "Let's take a quick look:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc7f33e",
      "metadata": {
        "id": "abc7f33e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "plt.scatter(trace2['mu'],trace2['sigma'])\n",
        "                   \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9751d6b7",
      "metadata": {
        "id": "9751d6b7"
      },
      "source": [
        "In this case, they appear to be independent even as posterior.  But here is an example where they may not be.\n",
        "\n",
        "\n",
        "### Example 3\n",
        "\n",
        "The data will be generated from the t distribution with $\\nu = 4$.  The location parameter will be $0$ and the scale parameter 1.\n",
        "\n",
        "\n",
        "For priors, we will say the location is uniform on $[-1,1]$, the scale is gamma with $\\alpha = 2$ and $\\beta = 1$\n",
        "\n",
        "For $\\nu$ will take lognormal with $\\sigma$ parmeter $1$ and $\\mu$ parameter 1.5\n",
        "\n",
        "\n",
        "Again, before we do anything, let us plot the priors for $\\nu$ and scale\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a956fc",
      "metadata": {
        "id": "89a956fc"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  10, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.lognorm.pdf(x, s= 1, scale = np.exp(1.5)), label=\"prior for $\\\\nu$\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for $\\\\nu$\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311041b7",
      "metadata": {
        "id": "311041b7"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  5, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.gamma.pdf(x, a=2, scale = 1), label=\"prior for scale\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for scale\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb369a55",
      "metadata": {
        "id": "eb369a55"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "\n",
        "N = 100\n",
        "\n",
        "data3 = ss.t.rvs(df = 4, size = N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fef7c39a",
      "metadata": {
        "id": "fef7c39a"
      },
      "outputs": [],
      "source": [
        "# Set up our model\n",
        "\n",
        "\n",
        "model_3 = pm.Model()\n",
        "\n",
        "with model_3:\n",
        "    \n",
        "    #define the priors\n",
        "    \n",
        "    loc = pm.Uniform('loc', lower= -1, upper = 1)\n",
        "    \n",
        "    scale = pm.Gamma('scale', alpha =2, beta =1)\n",
        "    \n",
        "    nu = pm.Lognormal('nu', sigma = 1, mu = 1.5)\n",
        "    \n",
        "    \n",
        "    #define the distribution of the observed variables\n",
        "    \n",
        "    Y_obs = pm.StudentT('Y_obs', nu = nu, mu = loc , sigma = scale,  observed=data3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1415457",
      "metadata": {
        "id": "d1415457"
      },
      "outputs": [],
      "source": [
        "# Generate samples\n",
        "\n",
        "with model_3:\n",
        "    \n",
        "    trace3 = pm.sample(1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6197ce",
      "metadata": {
        "id": "2e6197ce"
      },
      "outputs": [],
      "source": [
        "with model_3: \n",
        "    \n",
        "    az.plot_trace(trace3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc5e309",
      "metadata": {
        "id": "8cc5e309"
      },
      "source": [
        "We can compare the posteriors with the priors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6235af46",
      "metadata": {
        "id": "6235af46"
      },
      "outputs": [],
      "source": [
        "scale_kde =  ss.gaussian_kde(trace3['scale'])\n",
        "\n",
        "nu_kde =  ss.gaussian_kde(trace3['nu'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a483360a",
      "metadata": {
        "id": "a483360a"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  10, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.lognorm.pdf(x, s= 1, scale = np.exp(1.5)), label=\"prior for $\\\\nu$\")\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(x, nu_kde(x), label=\"Posterior KDE\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for $\\\\nu$\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375731c6",
      "metadata": {
        "id": "375731c6"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0.001,  5, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "\n",
        "plt.plot(x, ss.gamma.pdf(x, a=2, scale = 1), label=\"prior for scale\")\n",
        "\n",
        "plt.plot(x, scale_kde(x), label = \"posterior KDE\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"PDF for scale\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16542a6b",
      "metadata": {
        "id": "16542a6b"
      },
      "source": [
        "Now let's see how the scale and $nu$ distributions are depdendent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9b62d8",
      "metadata": {
        "id": "dc9b62d8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "plt.scatter(trace3['nu'],trace3['scale'])\n",
        "\n",
        "plt.xlabel(\"$\\\\nu$\")\n",
        "\n",
        "plt.ylabel(\"scale\")\n",
        "                   \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8142563",
      "metadata": {
        "id": "a8142563"
      },
      "source": [
        "We see that for larger values of $\\nu$ go with larger values of scale.  But the relationship is clearly not linear."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ecdf965",
      "metadata": {
        "id": "2ecdf965"
      },
      "source": [
        "#### Impact of sample size\n",
        "\n",
        "There are two sample sizes here\n",
        "\n",
        "1. The sample size of the actual data\n",
        "\n",
        "2. The sample size we generate from the posterior\n",
        "\n",
        "\n",
        "Making these larger will do different things depending on which one is made larger.\n",
        "\n",
        "Making the data sample larger, will \"tighten\" the posteriors.  We will know more.\n",
        "\n",
        "Making the MCMC sample larger will not change the posterior, but simply tell us more about it.\n",
        "\n",
        "Let's do each in turn for the previous example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a9afbc",
      "metadata": {
        "id": "f3a9afbc"
      },
      "outputs": [],
      "source": [
        "# Make data size larger\n",
        "\n",
        "N = 1000\n",
        "\n",
        "data3big = ss.t.rvs(df = 4, size = N)\n",
        "\n",
        "\n",
        "model_3big = pm.Model()\n",
        "\n",
        "with model_3big:\n",
        "    \n",
        "    #define the priors\n",
        "    \n",
        "    loc = pm.Uniform('loc', lower= -1, upper = 1)\n",
        "    \n",
        "    scale = pm.Gamma('scale', alpha =2, beta =1)\n",
        "    \n",
        "    nu = pm.Lognormal('nu', sigma = 1, mu = 1.5)\n",
        "    \n",
        "    \n",
        "    #define the distribution of the observed variables\n",
        "    \n",
        "    Y_obs = pm.StudentT('Y_obs', nu = nu, mu = loc , sigma = scale,  observed=data3big)\n",
        "\n",
        "    \n",
        "    trace3big = pm.sample(1000)\n",
        "    \n",
        "    az.plot_trace(trace3big)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95eb8662",
      "metadata": {
        "id": "95eb8662"
      },
      "outputs": [],
      "source": [
        "with model_3:\n",
        "\n",
        "    trace3biggersample = pm.sample(4000)\n",
        "    \n",
        "    az.plot_trace(trace3biggersample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42b4917",
      "metadata": {
        "id": "d42b4917"
      },
      "source": [
        "## Comment on priors\n",
        "\n",
        "Historically, one of the objections to Baysian analysis is that it required specifying the prior - which is \"subjective\".  If our priors are strong (small variance), them they will have a lot influence on our posteriors.  That isn't necessarily bad, but it could be if we are misinformed when setting our priors.\n",
        "\n",
        "On the other hand, if we have weak priors, we may be failing to utilize information we already have.  Having said that, as long as the prior is not too strong and we have a fair amount of data, the prior actually does not matter that much.  \n",
        "\n",
        "\n",
        "I will illustrate witt an example.    We will generate data from the exponential distribution $\\lambda = 1$ and specify two priors for the $\\lambda$ parameter.\n",
        "\n",
        "Prior 1:  Normal with mean of $1.1$ and standard deviation of $0.1$\n",
        "\n",
        "Prior 2:  Uniform on $[0.5,2]$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7992a05",
      "metadata": {
        "id": "c7992a05"
      },
      "outputs": [],
      "source": [
        "N = 1000\n",
        "\n",
        "data = ss.expon.rvs(size = N)\n",
        "\n",
        "prior_ex1 = pm.Model()\n",
        "\n",
        "with prior_ex1:\n",
        "    \n",
        "    lam1 = pm.Normal('lam1', mu = 1.1, sigma = 0.1)\n",
        "    \n",
        "    \n",
        "    Yobs = pm.Exponential(\"Yobs\", lam = lam1, observed = data)\n",
        "    \n",
        "    trace1 = pm.sample(1000)\n",
        "    \n",
        "    \n",
        "    az.plot_posterior(trace1, figsize = (15,10))\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914f0876",
      "metadata": {
        "id": "914f0876"
      },
      "outputs": [],
      "source": [
        "N = 1000\n",
        "\n",
        "data = ss.expon.rvs(size = N)\n",
        "\n",
        "prior_ex2 = pm.Model()\n",
        "\n",
        "with prior_ex2:\n",
        "    \n",
        "    lam2 = pm.Uniform('lam2', lower=0.5, upper= 2.0)\n",
        "    \n",
        "    \n",
        "    Yobs = pm.Exponential(\"Yobs\", lam = lam2, observed = data)\n",
        "    \n",
        "    trace2 = pm.sample(1000)\n",
        "    \n",
        "    \n",
        "    az.plot_posterior(trace2, figsize = (15,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1a07ea",
      "metadata": {
        "id": "5b1a07ea"
      },
      "source": [
        "So, even though we started with two quite different priors, we end up in a similar place.\n",
        "\n",
        "\n",
        "This leads to some general quidelines when setting priors:\n",
        "\n",
        "\n",
        "1. Don't over-specify\n",
        "2. The more data you have, the less important the prior.\n",
        "3. The reasonable support of the prior should include all plausible parameter values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f3d31d",
      "metadata": {
        "id": "88f3d31d"
      },
      "source": [
        "## Some Real Data\n",
        "\n",
        "We will get log-returns for the S&P500 and fit them to\n",
        "\n",
        "\n",
        "1) Normal Distribution\n",
        "\n",
        "2) Student T\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d18c8e3",
      "metadata": {
        "id": "9d18c8e3"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f879d2",
      "metadata": {
        "id": "63f879d2"
      },
      "outputs": [],
      "source": [
        "SP500 = yf.download('^GSPC',start = \"2015-01-01\", end = \"2022-03-01\")\n",
        "\n",
        "SP500['lret'] = np.log(SP500.Close).diff()\n",
        "\n",
        "SP500 = SP500.dropna()\n",
        "\n",
        "SP500.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a77f393",
      "metadata": {
        "id": "7a77f393"
      },
      "source": [
        "### Lognormal return model\n",
        "\n",
        "We will frame the return and standard deviation parameters in annualized terms.  Let's set the priors but with no too much confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeab5f13",
      "metadata": {
        "id": "aeab5f13"
      },
      "outputs": [],
      "source": [
        "SPM1 = pm.Model()\n",
        "\n",
        "\n",
        "with SPM1:\n",
        "    \n",
        "    mu_a = pm.Normal('mu_a', mu = .05, sigma = .05)\n",
        "    vol = pm.Uniform('vol', lower = .001, upper = .5)\n",
        "    \n",
        "    #we make them daily \n",
        "    \n",
        "    mu_d = mu_a/252\n",
        "    \n",
        "    vol_d = vol/np.sqrt(252)\n",
        "    \n",
        "    \n",
        "    # here is the model distribution\n",
        "    \n",
        "    Y = pm.Normal('Y', mu = mu_d, sigma = vol_d, observed = SP500.lret)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebae1c14",
      "metadata": {
        "id": "ebae1c14"
      },
      "outputs": [],
      "source": [
        "with SPM1:\n",
        "    trace1 = pm.sample(2000)\n",
        "    \n",
        "    az.plot_posterior(trace1, figsize = (15,10))\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96653bb4",
      "metadata": {
        "id": "96653bb4"
      },
      "source": [
        "### Comment\n",
        "\n",
        "Note that there is quite a bit of uncertainty around the mean return parameter.    The volatility, not so much. \n",
        "\n",
        "This matters if you are optimizing portfolios - because the expected return is a key input.\n",
        "\n",
        "\n",
        "### Student T version\n",
        "\n",
        "\n",
        "\n",
        "Location will be the same prior as above.  For scale, the same (since if $\\nu$ is large, scale is just vol.\n",
        "\n",
        "For $\\nu$, let's just take uniform on $[1,30]$.  There might be better choices, but we can defend this as follows:\n",
        "\n",
        "If the degrees freedom is above $30$ this is basically the normal distribution. \n",
        "\n",
        "If the degree freedom is $1$ or less, the first moment is undefined, so we are talking very heavy returns. We should however take note if the posterior is showing probabilities near $1$ as being reasonably high. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392a3b7f",
      "metadata": {
        "id": "392a3b7f"
      },
      "outputs": [],
      "source": [
        "SPM2 = pm.Model()\n",
        "\n",
        "\n",
        "with SPM2:\n",
        "    \n",
        "    mu_a = pm.Normal('mu_a', mu = .05, sigma = .05)\n",
        "    scale_a = pm.Uniform('scale_a', lower = .001, upper = .5)\n",
        "    \n",
        "    \n",
        "    nu = pm.Uniform('nu', lower = 1, upper = 30)\n",
        "    \n",
        "    #we make them daily \n",
        "    \n",
        "    mu_d = mu_a/252\n",
        "    \n",
        "    scale = scale_a/np.sqrt(252)\n",
        "    \n",
        "    \n",
        "    # here is the model distribution\n",
        "    \n",
        "    Y = pm.StudentT('Y',nu = nu,  mu = mu_d, sigma = scale, observed = SP500.lret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f10da1",
      "metadata": {
        "id": "22f10da1"
      },
      "outputs": [],
      "source": [
        "with SPM2:\n",
        "    trace2 = pm.sample(2000)\n",
        "    \n",
        "    az.plot_posterior(trace2, figsize = (15,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb8a61f",
      "metadata": {
        "id": "bcb8a61f"
      },
      "source": [
        "### Comments\n",
        "\n",
        "Interestingly, the view on the \"mean\" quite a but more optimistic.  This is probably (speculating) because the in log-normal model, a lower mean is required to deal with large negative returns. \n",
        "\n",
        "Note that the standard deviation of returns is $s_a \\times \\sqrt{\\frac{\\nu}{\\nu-2}}$ where $s_a$ is the scale here).  \n",
        "\n",
        "Let's create that data set and see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42cb47f5",
      "metadata": {
        "id": "42cb47f5"
      },
      "outputs": [],
      "source": [
        "vols = trace2['scale_a']  * np.sqrt( trace2['nu']/(trace2['nu']-2)   )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c105f5a",
      "metadata": {
        "id": "1c105f5a"
      },
      "source": [
        "The error message means that some of the sampled $\\nu$ values are less than $2$. We place the data into a data frame and delete rows that are inconvenient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee544ab",
      "metadata": {
        "id": "9ee544ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "t2data = pd.DataFrame([trace2['scale_a'], trace2['nu']]).T\n",
        "\n",
        "t2data.columns = ['scale', 'nu']\n",
        "\n",
        "t2data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8f11f0",
      "metadata": {
        "id": "ce8f11f0"
      },
      "outputs": [],
      "source": [
        "t2data['vol'] = t2data.scale * np.sqrt(t2data.nu/(t2data.nu -2) ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bb4ee4",
      "metadata": {
        "id": "a1bb4ee4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "plt.boxplot(t2data.vol.dropna())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1ed776",
      "metadata": {
        "id": "2c1ed776"
      },
      "outputs": [],
      "source": [
        "# Getting some more descriptive data \n",
        "\n",
        "t2data.vol.median(), t2data.vol.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06f46c1",
      "metadata": {
        "id": "e06f46c1"
      },
      "outputs": [],
      "source": [
        "np.percentile(t2data.vol.dropna(),25 ), np.percentile(t2data.vol.dropna(),75 )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1561a28",
      "metadata": {
        "id": "d1561a28"
      },
      "source": [
        "We see that the volatility is actually higher typically in this model than in the lognormal.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a65d55c",
      "metadata": {
        "id": "9a65d55c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Week 12.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}