{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhong338/MFM-FM5222/blob/main/Week7_HatMatrixandLev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261ec33f",
      "metadata": {
        "id": "261ec33f"
      },
      "source": [
        "# FM5222\n",
        "# The Hat Matrix and Leverage\n",
        "\n",
        "\n",
        "The context here is multi-variable linear regression which in general can be written in the form:\n",
        "\n",
        "$$Y = X \\mathbf{b}  + \\mathbf{\\epsilon}$$\n",
        "\n",
        "Where \n",
        "\n",
        "$X$ is a $N \\times p$ (or  $N \\times p+1$ if there is an intercept) data matrix representing $N$ obervations of $p$ (prediction) variables also called features.  \n",
        "\n",
        "$b$ is a vector of regression coefficients.\n",
        "\n",
        "$Y$ is the observed response variable (or target).\n",
        "\n",
        "$\\mathbf{\\epsilon}$ is a vector of residual noise assumed to be $\\mathcal{N}(0,\\sigma^2)$.\n",
        "\n",
        "Note that previosly we used $A$ for the matrix $X$, but we use $X$ now just to be more consistent with the text and most other presentations.\n",
        "\n",
        "Also, we previusly wrote this $ X \\mathbf{b} = Y  + \\mathbf{\\epsilon}$.  We also change this for greater notational consistency.\n",
        "\n",
        "\n",
        "Recall that the least-square fit (which is also the MLE) for the coeffiecients is given\n",
        "\n",
        "$$\\hat{b} = (X^TX)^{-1} X^T Y$$\n",
        "\n",
        "For each observation $i$, we can then calculate the fit values of $Y_i$ via \n",
        "\n",
        "$$\\hat{Y}_i = \\sum_k X_{i,k}\\hat{b}_k$$\n",
        "\n",
        "And the regresssed residual\n",
        "\n",
        "$$\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i $$\n",
        "\n",
        "\n",
        "We observe then that in matrix notation, we have\n",
        "\n",
        "$$\\hat{Y} = X  \\hat{b} =  X(X^TX)^{-1} X^T Y$$\n",
        "\n",
        "\n",
        "This means that each estimate $\\hat{Y}$ is a linear combination of the observed $Y$ values.  And this linear com bination is given by the matrix\n",
        "\n",
        "$$H = X(X^TX)^{-1} X^T $$\n",
        "\n",
        "\n",
        "This matrix is calles the \"hat matrix\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28d5c1f",
      "metadata": {
        "id": "f28d5c1f"
      },
      "source": [
        "## Hat Matrix\n",
        "\n",
        "Let us a take a look at this matrix for a minute. \n",
        "\n",
        "* It is $N \\times N$ but will be rank $p$ (or $p +1$ - we will just use $p$ for the duration).\n",
        "\n",
        "   We can see this because the matatrix $X^TX$ is a $p\\times p$ invertible matrix.\n",
        "   \n",
        "* The trace of $H$ is $p$.   Why is this?\n",
        "\n",
        "    Recall that the trace of a square matrix is the sum along the diagonals. Two key facts about the trace of a matrix are\n",
        "    \n",
        "    1. $\\mathrm{tr}(BAB^{-1}) = \\mathrm{tr}(A)$ .  That is, the trace is unaffected by similarity transformations.\n",
        "    2. If $AB$ is square, then $\\mathrm{tr}(AB) = \\mathrm{tr}(BA)$ \n",
        "    \n",
        "Using these\n",
        "\n",
        "$$\\mathrm{tr}(H) = \\mathrm{tr}\\left(X(X^TX)^{-1} X^T \\right)\\\\\n",
        "= \\mathrm{tr}\\left(X^T X(X^TX)^{-1} \\right)\\\\\n",
        "=\\mathrm{tr}(I_p) \\\\\n",
        "= p$$\n",
        "\n",
        "where $I_p$ is the $p\\times p$ identity matrix.\n",
        "\n",
        "\n",
        "\n",
        "Why do we care?  Because we want to understand how much each observation $Y_i$ impacts the estimate $\\hat{Y}_i$.\n",
        "\n",
        "\n",
        "In particular,\n",
        "\n",
        "$\\hat{Y}_i = \\sum_k H_{i,j}Y_k = H_{i,i} Y_i + \\sum_{k\\neq i} H_{i,j}Y_k$\n",
        "\n",
        "so we see that the influence that $Y_i$ itself has on its own estimate $\\hat{Y}_i$ is determined by the diagonal element of $H$, $H_{i,i}$.\n",
        "\n",
        "We call the valaue $H_{i,i}$ the *leverage* of the $i$th observation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2989ee84",
      "metadata": {
        "id": "2989ee84"
      },
      "source": [
        "## Leverage\n",
        "\n",
        "Consider the vector $\\hat{Y} = HY$.  Itss covariance matrix will be given by\n",
        "\n",
        "\n",
        "\n",
        "$$\\mathrm{Cov}(\\hat{Y}) = H\\mathrm{Cov}(Y)H^T\\\\\n",
        "= H\\sigma_{\\epsilon}^2I_N H^T\\\\\n",
        "\\sigma_{\\epsilon}^2HH^T$$\n",
        "\n",
        "However, we note that $H^T = H$ and therefore\n",
        "\n",
        "$$HH^T = X(X^TX)^{-1} X^T X(X^TX)^{-1} X^T\\\\\n",
        " =X(X^TX)^{-1}I_p X^T\\\\\n",
        " = H$$\n",
        " \n",
        "Hence,\n",
        "\n",
        "$$\\mathrm{Cov}(\\hat{Y}) = \\sigma_{\\epsilon}^2 H$$.\n",
        "\n",
        "In particular, \n",
        "\n",
        "$$\\mathrm{Var}(\\hat{Y}_i) = \\sigma_{\\epsilon}^2 H_{i,i}$$\n",
        "\n",
        "This means that if the leverage is high, the standard error of the estimate is high.  For this reason, high leverage points can be points of concern.\n",
        "\n",
        "\n",
        "### How to interpret leverage\n",
        "\n",
        "\n",
        "For the $i$th observartion vector, $H_{i,i} = X_i^T (X^TX)^{-1} X_i$.  But  the matrix $(X^TX)^{-1}$ is a positive definite matrix which forces  $0 < H_{i,i}$\n",
        "\n",
        "Becaue $HH^T = H$, we can say that\n",
        "\n",
        "$$H_{i,i} = \\sum_j H_{i,j}^2 \\geq H_{i,i}^2$$\n",
        "\n",
        "and therefore $H_{i,i} \\leq 1$\n",
        "\n",
        "So we know that $0 < H_{i,i} \\leq 1$\n",
        "\n",
        "\n",
        "Because $\\mathrm{tr}(H) = p$, we know that $\\frac{1}{N}\\sum_{i=1}^N H_{i,i} =\\frac{p}{N}$ so the average leverage will be $\\frac{p}{N}$.  A high-leverage point will be above averge and a rule-of-thumb for concern is twice the average.  That is, we consider leverage to be high if\n",
        "\n",
        "$$H_{i,i} > 2\\frac{p}{N}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6acbaf",
      "metadata": {
        "id": "3a6acbaf"
      },
      "source": [
        "### Example\n",
        "\n",
        "We construct a fake data example where one of the $X$ observation will be high leverage.  We can then generate several different versions of $Y$ data consistent with the the same $X$ observations and we show that, for the high-leverage observation, $\\hat{Y}_i$ is highly variable.\n",
        "\n",
        "\n",
        "We will take $p= 3$ and $N = 12$ and omit an intercept.  The \"true model\" will have $\\mathrm{b}^T = (1,2,3)$ and $\\sigma_{\\epsilon} = 2$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72144032",
      "metadata": {
        "id": "72144032"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67869e26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67869e26",
        "outputId": "53a20778-50c1-438f-fbdd-52b990035e6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0. , -2. , -2. ],\n",
              "       [ 1. , -3. , -3. ],\n",
              "       [ 1.5, -0.5,  0. ],\n",
              "       [-1. ,  1. , -0.5],\n",
              "       [-2. ,  0.5,  2. ],\n",
              "       [ 0.5, -1.5, -2.5],\n",
              "       [-2.5, -2.5,  1. ],\n",
              "       [ 2. ,  1.5,  0.5],\n",
              "       [-3. , -1. ,  1.5],\n",
              "       [-0.5,  0. , -1. ],\n",
              "       [-1.5,  2. , -1.5],\n",
              "       [ 5.5,  5.5,  5.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "sigma_e = 2\n",
        "b = np.array([1,2,3])\n",
        "\n",
        "\n",
        "X = np.array([[ 0. , -2. , -2. ],[ 1. , -3. , -3. ],[ 1.5, -0.5,  0. ],[-1. ,  1. , -0.5],[-2. ,  0.5,  2. ],\\\n",
        "    [ 0.5, -1.5, -2.5],[-2.5, -2.5,  1. ],[ 2. ,  1.5,  0.5],[-3. , -1. ,  1.5],\\\n",
        "    [-0.5,  0. , -1. ],[-1.5,  2. , -1.5], [ 5.5,  5.5,  5.5]])\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c873f880",
      "metadata": {
        "id": "c873f880"
      },
      "source": [
        "Let's calculate the hat matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd03463",
      "metadata": {
        "id": "9cd03463"
      },
      "outputs": [],
      "source": [
        "H = X@np.linalg.pinv(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aadc42d3",
      "metadata": {
        "id": "aadc42d3"
      },
      "source": [
        "And not the leveragess of the observations.  Recall, that high leverage will be leverage higher than\n",
        "\n",
        "$$2 \\frac{p}{N} = .5$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97d788a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97d788a",
        "outputId": "49d69d86-35e6-466c-fb74-d7d6d8404ac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.10363453, 0.32994599, 0.08514144, 0.12533187, 0.19433764,\n",
              "       0.13705026, 0.34697427, 0.07587201, 0.28206537, 0.03943514,\n",
              "       0.51586103, 0.76435045])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "np.diag(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f54eba",
      "metadata": {
        "id": "24f54eba"
      },
      "source": [
        "We can see that the last observation is a high leverage point (by design of course!).  The second to last observation is also on the high side (not by design)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad997dc",
      "metadata": {
        "id": "2ad997dc"
      },
      "source": [
        "Now generate $100$ different samplings of $Y$ under the notion that $Y = Xb + \\epsilon$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c77d7fc",
      "metadata": {
        "id": "2c77d7fc"
      },
      "outputs": [],
      "source": [
        "epsilons = np.random.normal(scale = sigma_e, size = [12,100])\n",
        "\n",
        "Ys = np.reshape(X@b, [12,1])@np.ones([1,100]) + epsilons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446ec9b3",
      "metadata": {
        "id": "446ec9b3"
      },
      "source": [
        "We can now creates 100 different estimates of $\\hat{Y}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2e4072",
      "metadata": {
        "id": "8b2e4072"
      },
      "outputs": [],
      "source": [
        "Yhats = H@Ys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69799694",
      "metadata": {
        "id": "69799694"
      },
      "source": [
        "We are interested in the standard deviation  of each element of $\\hat{Y}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3965098d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3965098d",
        "outputId": "94e8b505-7c53-499f-faf4-a4c367b98bff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.62379145, 1.13661485, 0.59029915, 0.71708115, 0.89647068,\n",
              "       0.71346189, 1.24664983, 0.57135274, 1.11164715, 0.38566286,\n",
              "       1.45538315, 1.63999593])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "np.std(Yhats, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fd1740",
      "metadata": {
        "id": "61fd1740"
      },
      "source": [
        "We compare this with the predicted (theoretical) values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e78235e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e78235e",
        "outputId": "7c982a19-025a-4ad8-8347-7bb9b9d91d7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.64384636, 1.1488185 , 0.58358014, 0.70804482, 0.88167485,\n",
              "       0.740406  , 1.17809045, 0.5508975 , 1.06219653, 0.39716564,\n",
              "       1.43646932, 1.74854277])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sigma_e* np.sqrt(np.diag(H))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f82020",
      "metadata": {
        "id": "72f82020"
      },
      "source": [
        "### Summaary\n",
        "\n",
        "* Each estimate $\\hat{Y}_i$ is a linear combination of the the observed values of $Y$. \n",
        "\n",
        "* The linear combination is given by the hat matrix $H = X(X^TX)^{-1} X$\n",
        "\n",
        "* The diagonal elements of $H$ measure how much the estimate $\\hat{Y}_i$ depends directly on $Y_i$ and is called the leverage.\n",
        "\n",
        "* Leverage more than twice the average leverage of $2 \\frac{p}{N}$ is considered high leverge and is potentially a problem.\n",
        "\n",
        "* The standard error of $\\hat{Y}_i$ is $\\sigma_{\\epsilon} \\sqrt{H_{i,i}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "498e35a6",
      "metadata": {
        "id": "498e35a6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "HatMatrixandLev.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}