{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhong338/MFM-FM5222/blob/main/Week6_EMmethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5abac45",
      "metadata": {
        "id": "d5abac45"
      },
      "source": [
        "# MLE via EM\n",
        "\n",
        "In this short video, I will describe the basic concept and motivation for the EM algorithm.  Which can used to obtain MLE parameter estimates.\n",
        "\n",
        "\n",
        "\n",
        "## Motivation\n",
        "\n",
        "The general MLE problem can be stated as follows:\n",
        "\n",
        "We have $N$ IID observations of a Random Variable $X$ and we suppose that the governing distribution has parameter vector $\\mathbf{\\theta} = (\\theta_1, \\theta_2,..., \\theta_m)$ with PDF\n",
        "\n",
        "$$f_X(x) = f_{X}(x;\\theta)$$\n",
        "\n",
        "If our observations are $x_i$, then the log-liklihood of our observations given  $\\theta$ is \n",
        "\n",
        "$$\\ell(\\mathbf{\\theta}) = \\sum_{i=1}^N \\ln(f_X(x_i;\\mathbf{\\theta})\\$$\n",
        "\n",
        "We attempt to find the value of $\\mathbf{\\theta}$ that will maximize this.  This is the MLE for the parameters.\n",
        "\n",
        "In many cases, this maximization is numerically \"easy\" either because\n",
        "\n",
        "1. One can explicitly come up with a formula for the MLE (e.g. normal distribution case)\n",
        "2. The gradient of $\\ell(\\mathbf{\\theta})$ with respect to the $\\theta_k$s can be easily done analytically and numerically solutions for the zeros of the gradient are \"easy\" for a computer.\n",
        "3. The numerical maximization \"easy\" for a computer even if he gradient is not.\n",
        "\n",
        "\n",
        "However, there are many easily specified exampless where none of the above are true.  Nevertheless, we can obtain MLE estimates if we use enough of what we know about the distribution.\n",
        "\n",
        "\n",
        "Note also that in this presentation, $X$ can be vector valued. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8216c2",
      "metadata": {
        "id": "7f8216c2"
      },
      "source": [
        "## Discrete Mixture Example\n",
        "\n",
        "Consider the mixture model where with probability $p$, $X \\sim \\mathcal{N}(0,1)$ and with probability $(1-p), X \\sim \\mathcal{N}(0,\\sigma^2)$\n",
        "\n",
        "We assume we have $N$ obervations of $X$ but no information on $\\sigma$ or $p$.  Can we estimate thee parameters by MLE?  Note here we are taking $\\mathbf{\\theta} = (\\sigma,p)$\n",
        "\n",
        "\n",
        "\n",
        "### Typical approach\n",
        "\n",
        "\n",
        "We know that $$f_X(x;\\sigma,p) = p\\phi(x) + (1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x}{\\sigma}\\right)$$\n",
        "\n",
        "Hence\n",
        "\n",
        "$$\\ell(\\sigma,p) = \\sum_{i=1}^N \\ln\\left(  p\\phi(x_i) + (1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma}\\right)           \\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d50bf7",
      "metadata": {
        "id": "f3d50bf7"
      },
      "source": [
        "We next calculate the gradient.\n",
        "\n",
        "\n",
        "$$\\frac{\\partial \\ell(\\sigma,p)}{\\partial p} = \\sum_{i=1}^N \\frac{\\phi(x_i) -\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma}\\right) }{  p\\phi(x_i) + (1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma}\\right)           }$$\n",
        "\n",
        "$$\\frac{\\partial \\ell(\\sigma,p)}{\\partial \\sigma} = \\sum_{i=1}^N \\frac{(1-p)\\left(\\frac{-1}{\\sigma^2 } + \\frac{1}{\\sigma^2}\\phi\\left(\\frac{x_i}{\\sigma}\\right) \\right)}{  p\\phi(x_i) + (1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma}\\right)        }$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d69bea0",
      "metadata": {
        "id": "4d69bea0"
      },
      "source": [
        "While it may be true that these can be set to zero and the sytem numerically solved, the point here is that you can see how it can get messy.   The basic reason for this is that there are \"sums\" inside the log above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0753d5e4",
      "metadata": {
        "id": "0753d5e4"
      },
      "source": [
        "### EM approach\n",
        "\n",
        "EM stands for Expectation-Maximization.  It is an iterative proceedure that attempts to improve the estimates at each step via two substeps:  Expectation and Maximization (what did you think they would be called?)\n",
        "\n",
        "\n",
        "We suppose that at step $k$, we have estimates for $\\sigma_k$ and $p_k$ which we hope to improve.\n",
        "\n",
        "\n",
        "Let us call the results of each binomial outcome (for each observation $i$) $h_i$.  Namely, $h_i=1$ means $x_i$ came from distribution 1 and $h_i= 0$ means from distribution 2.  We *DO NOT KNOW* the values of $h_i$, they are latent (or hidden) variables. What *can* we say? \n",
        "\n",
        "Assuing our estimates are true, the joint distribution of $(X,h)$ is given by\n",
        "\n",
        "\n",
        "$f_{X,H}(x,h; \\sigma_k, p_k) = p_k\\mathbf{1_{h=1}}\\phi(x)  +(1-p_k) \\mathbf{1_{h=0}}\\frac{1}{\\sigma_k}\\phi\\left(\\frac{x}{\\sigma_k}\\right)$ \n",
        "\n",
        "So, by Bayes theorem,\n",
        "\n",
        "$$f_{H}(h;x,\\sigma, p) = \\frac{f_{X,H}(x,h; \\sigma_k, p_k)}{f_X(x;\\sigma_k, p_k)} \\\\\n",
        "=\\frac{p_k\\mathbf{1_{h=1}}\\phi(x)  + (1-p_k)\\mathbf{1_{h=0}}\\frac{1}{\\sigma_k}\\phi\\left(\\frac{x}{\\sigma_k}\\right)}{p_k\\phi(x) + (1-p_k)\\frac{1}{\\sigma_k}\\phi\\left(\\frac{x}{\\sigma_k}\\right)}\\\\\n",
        "=\\tilde{p}_k(x)\\mathbf{1}_{h=1} + (1- \\tilde{p}_k(x))\\mathbf{1}_{h=0}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\tilde{p_k}(x) = \\frac{p_k\\phi(x)}{p_k\\phi(x) + (1-p_k)\\frac{1}{\\sigma_k}\\phi\\left(\\frac{x}{\\sigma_k}\\right)}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6555f849",
      "metadata": {
        "id": "6555f849"
      },
      "source": [
        "Now the log-liklihood our unkown parameters is given by \n",
        "\n",
        "$$\\ell(\\sigma,p ; x_i,h_i) = \\ln\\left(f_{X,H}(x_i,h); \\sigma_k, p_k)\\right)\\\\\n",
        "=\\ln(p\\phi(x_i))\\mathbf{1}_{h_i=1}+ \\ln\\left((1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma} \\right)\\right) \\mathbf{1}_{h_i=0}$$\n",
        "\n",
        "\n",
        "In the E step,we 'calulate' the expected value of this using our previous estimates $(\\sigma_k, p_k)$ and the obervation $x_i$.  That is\n",
        "\n",
        "$$\\mathrm{E}[\\ell(\\sigma,p ; x_i,h_i)] = \\tilde{p}_k(x_i)\\ln(p\\phi(x_i)) + \\left(1-\\tilde{p}_k(x_i) \\right)\\ln\\left((1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma} \\right)\\right) $$\n",
        "\n",
        "\n",
        "Usisng the linearity of expectations, we have\n",
        "\n",
        "$$Q_k(\\sigma, p) = \\mathrm{E}[\\ell(\\sigma,p ; \\{x_i\\},h_i)] = \\sum_{i=1}^N \\left( \\tilde{p}_k(x_i)\\ln(p\\phi(x_i)) + \\left(1-\\tilde{p}_k(x_i) \\right)\\ln\\left((1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma} \\right)\\right)\\right)$$\n",
        "\n",
        "This is the $E$ Step.\n",
        "\n",
        "\n",
        "Next, we find the choices of $\\sigma$ and $p$ that maximize $Q_k$. These in turn become $\\sigma_{k+1}$ and $p_{k+1}$\n",
        "\n",
        "This is the $M$ step.\n",
        "\n",
        "Though at first glance, this may seem like a mess, but note that we no longer have sums inside the log in our maximization problem (as we did above).\n",
        "\n",
        "This is because we can calculate the $\\tilde{p}_k(x_i)$ so they are just numbers (not variables).  Hence when we take the partials, we first note that\n",
        "\n",
        "$$\\ln\\left((1-p)\\frac{1}{\\sigma}\\phi\\left(\\frac{x_i}{\\sigma} \\right)\\right)\\\\\n",
        "= \\ln(1-p) -\\ln(\\sigma) -\\frac{1}{2}\\ln(2\\pi) - \\frac{x_i^2}{2\\sigma^2}$$\n",
        "\n",
        "\n",
        "Hence,\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90348490",
      "metadata": {
        "id": "90348490"
      },
      "source": [
        "$$\\frac{\\partial Q_k(\\sigma, p)}{\\partial p} = \\sum_{i=1}^N\\left(\\tilde{p}_k(x_i)\\frac{1}{p} - \\frac{1-\\tilde{p}_k(x_i)}{1-p}    \\right)$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\frac{\\partial Q_k(\\sigma, p)}{\\partial \\sigma} =\\sum_{i=1}^N (1-\\tilde{p}_k(x_i))\\left( \\frac{-1}{\\sigma }    + \\frac{x_i^2}{\\sigma^3}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d05665",
      "metadata": {
        "id": "08d05665"
      },
      "source": [
        "From the first, we deduce that  \n",
        "\n",
        "$$p_{k+1} = \\frac{1}{N}\\sum_{i=1}^N \\tilde{p}_k(x_i)$$\n",
        "\n",
        "\n",
        "From the second,\n",
        "\n",
        "$$\\sigma_{k+1}^2 = \\frac{\\sum_{i=1}^N (1-\\tilde{p}_k(x_i)x_i^2}{\\sum_{i=1}^N (1-\\tilde{p}_k(x_i))}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea702e2b",
      "metadata": {
        "id": "ea702e2b"
      },
      "source": [
        "So we can see here, that in this special example, we have moved from a complicated to numerical solve, to a relatively straighforward iteration.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1354e4f5",
      "metadata": {
        "id": "1354e4f5"
      },
      "source": [
        "\n",
        "## THE POINT!\n",
        "\n",
        "In the case of the multivariate T distribution, this situation is similar but quite a bit more complex.  Nevertheless, the EM approach allows a compuationally not terrible iteration to replace an otherwise \"hard\" maximization problem.  (Recall that the T distribution is a continous mixture of normal distributions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3dd5abc",
      "metadata": {
        "id": "d3dd5abc"
      },
      "source": [
        "### Previous example with actual numbers.\n",
        "\n",
        "We will generate a sample of $1000$ points with $\\sigma = 3$ and $p = .8$.  We then obtain the MLE two ways:\n",
        "\n",
        "1. But using the traditional maximization. \n",
        "\n",
        "2. Using the EM iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f56794",
      "metadata": {
        "id": "49f56794"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as ss\n",
        "\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cc96f5",
      "metadata": {
        "id": "d5cc96f5"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "\n",
        "N = 1000\n",
        "p = .8\n",
        "sigma = 3\n",
        "\n",
        "hdata = ss.binom.rvs(1,p =p, size = N, random_state = 2222)  #unobserved latent variables\n",
        "\n",
        "xdata = hdata*ss.norm.rvs(size = N,random_state = 22) +   (1- hdata)*ss.norm.rvs(scale = 3, size = N,random_state = 111)  #observed variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3d4a84",
      "metadata": {
        "id": "5b3d4a84"
      },
      "outputs": [],
      "source": [
        "## Make log-liklihood function to be maximized\n",
        "\n",
        "\n",
        "\n",
        "llh =  lambda sig, p, data:  np.log( p*ss.norm.pdf(data) + (1-p)*ss.norm.pdf(data, scale = sig)).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e0bdc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2e0bdc3",
        "outputId": "f83fdaec-bd20-4be5-e399-2435fe63aeb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " final_simplex: (array([[2.94017231, 0.78573763],\n",
              "       [2.94024636, 0.78573848],\n",
              "       [2.94020302, 0.78575006]]), array([1808.20645346, 1808.20645352, 1808.20645353]))\n",
              "           fun: 1808.206453463979\n",
              "       message: 'Optimization terminated successfully.'\n",
              "          nfev: 87\n",
              "           nit: 49\n",
              "        status: 0\n",
              "       success: True\n",
              "             x: array([2.94017231, 0.78573763])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Maximize llh\n",
        "\n",
        "F = lambda x: -llh(x[0],x[1], xdata)\n",
        "\n",
        "x0 = np.array([1.5,.5])\n",
        "\n",
        "\n",
        "result = minimize(F, x0, method = \"Nelder-Mead\")\n",
        "\n",
        "result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41fb41a2",
      "metadata": {
        "id": "41fb41a2"
      },
      "source": [
        "Comment, if we don't use Nelder-Mead, scipy has difficulty.  Consistent with observation that this can be hard to do.\n",
        "\n",
        "#### EM method.\n",
        "\n",
        "We need to define our $\\tilde{p}_k(x_i)$ functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce1c66f",
      "metadata": {
        "id": "cce1c66f"
      },
      "outputs": [],
      "source": [
        "ptilde = lambda x, sigk,pk:   pk*ss.norm.pdf(x)/( pk*ss.norm.pdf(x) + (1-pk)*ss.norm.pdf(x, scale = sigk) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c791b",
      "metadata": {
        "id": "c08c791b"
      },
      "source": [
        "Now we set up iteration lazily setting the number of iteration to $100$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7b3d8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a7b3d8d",
        "outputId": "a6081203-390c-490a-95d0-a1a7dce1bc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sigma estimate is  2.9615629376560957\n",
            "The p estimate is  0.7907720809887262\n"
          ]
        }
      ],
      "source": [
        "M = 100\n",
        "\n",
        "sigmas = np.ones(M+1)\n",
        "ps = np.ones(M+1)\n",
        "\n",
        "sigmas[0] = 1.5\n",
        "ps[0] = 0.5\n",
        "\n",
        "for k in range(M):\n",
        "    \n",
        "    #ps[k+1] = ptilde(xdata, sigmas[k], ps[k]).mean()\n",
        "    \n",
        "    ps[k+1] = ptilde(xdata, 3, ps[k]).mean()\n",
        "    \n",
        "    \n",
        "    sigmas[k+1] = np.sqrt(np.sum((1- ptilde(xdata, sigmas[k], ps[k]))*xdata**2)/\\\n",
        "                          np.sum(1- ptilde(xdata, sigmas[k], ps[k])))\n",
        "                                                                               \n",
        "                                                                               \n",
        "print(\"the sigma estimate is \", sigmas[-1])\n",
        "print(\"The p estimate is \", ps[-1])\n",
        "                                                                               \n",
        "                                                                                                                                                                                                                               \n",
        "                                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d001ac6",
      "metadata": {
        "id": "2d001ac6"
      },
      "source": [
        "The parameterss are close but slightly different, which we will attritube to numerical round off.  We can compare however."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1b2c9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be1b2c9c",
        "outputId": "f7df35b6-44d0-428c-9c2e-c9848d05b849"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1808.206453463979"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# First approach\n",
        "llh(result.x[0], result.x[1], xdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0535e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d0535e1",
        "outputId": "58c84372-2043-40c7-b185-44eefec38de6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1808.2227351281122"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# EM method\n",
        "\n",
        "llh(sigmas[-1],ps[-1], xdata)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "EMmethod.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}